pull based v/s push based:
Ansible         puppets,Terraform

Defualt Inventory Directory -> /etc/ansible/hosts

Custom Inventory-> (Setup: 1 Main server and 3 target server)
pwd (/home/vunet/vprofile)-> inventory , client.pem (chmod 400)

Ansible is an simple automation tool 
1. Automation: any system Automation 
2. Change Management: Production Server changes 
3. Provisioning: Setup Servers from scratch/ cloud provisioning 
4. Orchestration: Large scale automcation framework

Ansible is Simple: 
No Agents : Target Machines/service are accessby SSH,winrm & API
No Databases: YAML, INI & Texts
No complex Setup: It's just a python library
NO Ressidual Software: Push Python package -> Execute > Return Output
API -> URL/Restful Calls(e:g Cloud), Shell commands, Scripts 

Ansible Connections:
Linux - ssh, windows - winrm , Cloud -> API, Network Devices  -> Python Libraries 
> It dosn't use any service just uses python modules and executes the steps in the yaml in the target machines 

=============== Ansible Architecture ================================

                      ANSIBLE ARCHITECTURE OVERVIEW
                      --------------------------------

                       +------------------------+
                       |     Control Node       |
                       |  (Where Ansible runs)  |
                       +------------------------+
                                 |
                                 | SSH (or WinRM for Windows)
                                 |
        -------------------------------------------------------------
        |                      |                      |             |
+----------------+    +----------------+    +----------------+    +----------------+
| Managed Node 1 |    | Managed Node 2 |    | Managed Node 3 |    | Managed Node N |
| (Linux/Win/mac)|    | (Linux/Win/mac)|    | (Linux/Win/mac)|    | (Linux/Win/mac)|
+----------------+    +----------------+    +----------------+    +----------------+

Legend:
- Control Node: Host where Ansible is installed.
- Managed Nodes: Target servers where Ansible executes tasks.
- No agent needed on managed nodes.
- Communication via SSH (or WinRM for Windows).

Ansible Workflow:
------------------
1. Write Playbooks â†’ YAML files with desired configurations/tasks.
2. Inventory File  â†’ List of managed hosts (IP/hostname/group).
3. Execute Commands or Playbooks from Control Node.
4. Ansible uses SSH to connect and run modules on target hosts.
5. Modules return output â†’ Ansible shows results.

Folder Structure Example:
-------------------------
project/
â”œâ”€â”€ inventory.ini           # List of managed hosts
â”œâ”€â”€ playbook.yml            # Main playbook
â”œâ”€â”€ roles/                  # Reusable task collections
â”‚   â””â”€â”€ webserver/
â”‚       â”œâ”€â”€ tasks/main.yml
â”‚       â”œâ”€â”€ handlers/main.yml
â”‚       â”œâ”€â”€ templates/
â”‚       â””â”€â”€ files/
â””â”€â”€ group_vars/
    â””â”€â”€ all.yml             # Variables used in playbooks

Commands Example:
-----------------
# Check inventory connectivity:
ansible -i inventory.ini all -m ping

# Run a playbook:
ansible-playbook -i inventory.ini playbook.yml

# Ad-hoc command to install nginx:
ansible all -i inventory.ini -m apt -a "name=nginx state=present" -b


Note: All the playbooks that we right will be converted into python and it will run in the target servers 
    
    
vi inventory: 
all:
  hosts:
    web01:
      ansible_host: 10.0.1.118
      ansible_user: ec2-user
      ansible_ssh_private_key_file: client.pem  # (it should be in the current dir)
    web02:
      ansible_host: 10.0.1.99
      ansible_user: ec2-user
      ansible_ssh_private_key_file: client.pem  # (it should be in the current dir)
    db01:
      ansible_host: 10.0.1.240
      ansible_user: ec2-user
      ansible_ssh_private_key_file: client.pem  # (it should be in the current dir)

  children:
    webservers:  # Group 1
      hosts:
        web01:
        web02:
    dbservers:  # Group 2
      hosts:
        db01:
    dc_oregon:  # Group 3 -> This means both the groups will be affected if we are using this children group
      children:
        webservers:
        dbservers:

	
$ ansible web01 -m ping -i inventory
$ ansible all -m ping -i inventory
$ ansible webservers -m ping -i inventory
---> you will get success if all these things goes rights


ubuntu@ip-172-31-95-88:~/ansible/writing-inventory-file$ ansible web01 -m ping -i inventory.yaml
The authenticity of host '172.31.94.128 (172.31.94.128)' can't be established.
ED25519 key fingerprint is SHA256:vqVmA7pvWv8XMDjo/JhgL+EQ6UyB0HmR24k8WH+yKHE.
This key is not known by any other names.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
web01 | UNREACHABLE! => {
    "changed": false,
    "msg": "Failed to connect to the host via ssh: Warning: Permanently added '172.31.94.128' (ED25519) to the list of known hosts.\r\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\n@         WARNING: UNPROTECTED PRIVATE KEY FILE!          @\r\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\nPermissions 0664 for 'client.pem' are too open.\r\nIt is required that your private key files are NOT accessible by others.\r\nThis private key will be ignored.\r\nLoad key \"client.pem\": bad permissions\r\nubuntu@172.31.94.128: Permission denied (publickey).",
    "unreachable": true
}


--> This will ask for host key checking for the fingerprint for all the new target servers that you ill add in the inventory. How to resolve ?? 
How to disable host key checking since it asks fingerprint everytime : /etc/ansible/ansible.cfg -> host_key_checking=False
=

root@ip-172-31-95-88:/home/ubuntu/ansible/writing-inventory-file# ansible web01 -m ping -i inventory.yaml
[WARNING]: Platform linux on host web01 is using the discovered Python interpreter at
/usr/bin/python3.12, but future installation of another Python interpreter could change the meaning of
that path. See https://docs.ansible.com/ansible-
core/2.18/reference_appendices/interpreter_discovery.html for more information.
web01 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.12"
    },
    "changed": false,
    "ping": "pong"
}

----- Pinging the Groups: --------------- 
root@ip-172-31-95-88:/home/ubuntu/ansible/writing-inventory-file# ansible webservers -m ping -i inventory.yaml
[WARNING]: Platform linux on host web01 is using the discovered Python interpreter at
/usr/bin/python3.12, but future installation of another Python interpreter could change the meaning of
that path. See https://docs.ansible.com/ansible-
core/2.18/reference_appendices/interpreter_discovery.html for more information.
web01 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.12"
    },
    "changed": false,
    "ping": "pong"
}
[WARNING]: Platform linux on host web02 is using the discovered Python interpreter at
/usr/bin/python3.12, but future installation of another Python interpreter could change the meaning of
that path. See https://docs.ansible.com/ansible-
core/2.18/reference_appendices/interpreter_discovery.html for more information.
web02 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.12"
    },
    "changed": false,
    "ping": "pong"
}

root@ip-172-31-95-88:/home/ubuntu/ansible/writing-inventory-file# ansible dc_oregon -m ping -i inventory.yaml
[WARNING]: Platform linux on host web01 is using the discovered Python interpreter at
/usr/bin/python3.12, but future installation of another Python interpreter could change the meaning of
that path. See https://docs.ansible.com/ansible-
core/2.18/reference_appendices/interpreter_discovery.html for more information.
web01 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.12"
    },
    "changed": false,
    "ping": "pong"
}
[WARNING]: Platform linux on host web02 is using the discovered Python interpreter at
/usr/bin/python3.12, but future installation of another Python interpreter could change the meaning of
that path. See https://docs.ansible.com/ansible-
core/2.18/reference_appendices/interpreter_discovery.html for more information.
web02 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.12"
    },
    "changed": false,
    "ping": "pong"
}
[WARNING]: Platform linux on host db01 is using the discovered Python interpreter at
/usr/bin/python3.12, but future installation of another Python interpreter could change the meaning of
that path. See https://docs.ansible.com/ansible-
core/2.18/reference_appendices/interpreter_discovery.html for more information.
db01 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.12"
    },
    "changed": false,
    "ping": "pong"
}

$ ansible all -m ping -i inventory.yaml --> This will ping all the hosts 


============= VARIABLES in Inventory ====================
variable has the top prority when checking the login credentials. If not provided then it will go and check in the childrens then it will execute.

For Example: 
all:
  hosts:
    web01:
      ansible_host: 10.0.1.118
      ansible_user: ec2-user
      ansible_ssh_private_key_file: client.pem  # (it should be in the current dir)
    web02:
      ansible_host: 10.0.1.99
      ansible_user: ec2-user
      ansible_ssh_private_key_file: client.pem  # (it should be in the current dir)
    db01:
      ansible_host: 10.0.1.240

  children:
    webservers:  # Group 1
      hosts:
        web01:
        web02:
    dbservers:  # Group 2
      hosts:
        db01:
    dc_oregon:  # Group 3 -> This means both the groups will be affected if we are using this children group
      children:
        webservers:
        dbservers:
	  vars: 
	    ansible_user: ec2-user
        ansible_ssh_private_key_file: client.pem  # (it should be in the current dir)
-----------------------------

Json V/S YAML -> How to convert?? 

{Devops: ["AWS", "Terrform", "Ansible"], "Learning": ["Linux","Jenkins,"K8s"], "Development": {"Java": "Spring", "React": "Next.js"}}

The same json in yaml below

Devops:
 - AWS
 - Terraform 
 - Ansible 
Learning:
 - Linux
 - Jenkins
 - K8s
Development:
  Java: Spring 
  React: Next.js


================ AD HOC Commands =================
ad hoc commands are greate for taksk you repeat rearely. For example if you want to powerr off all the machines in your lab for christmas vacation, You clould execute a quick one liner ansible without writing a playbook. An ad hoc commands looks like this .

$ ansible [pattern] -m [module] -a "[module options]"
$ ansible atlanta -a "sbin/reboot"

There are lot of modules which can be used to automate the tasks:

#copy files 
----------------------
$ ansible atlanta -m ansible.builtin.copy -a "src=/etc/hosts dest=/tmp/hosts" 
# Change ownership and permissions on files 
$ ansible atlanta -m ansible.builtin.file -a "dest/srv/foo/a.txt mode=600"
$ ansible webservers -m ansible.builtin.file -a "dest=/src/foo/b.txt mode=600 onwer=arjun group=arjun"

#Install Services -> If will maintain the state what we mention in the state 
----------------------
$ ansible web01 -m ansible.builtin.yum -a "name=http state=present" -i inventory.yaml --become (To run with sudo privilages)

root@ip-172-31-95-88:/home/ubuntu/ansible/writing-inventory-file# ansible web01 -m ansible.builtin.yum -a "name=httpd state=present" -i inventory.yaml --become
web01 | FAILED! => {
    "ansible_facts": {
        "pkg_mgr": "apt"
    },
    "changed": false,
    "msg": [
        "Could not detect which major revision of dnf is in use, which is required to determine module backend.",
        "You should manually specify use_backend to tell the module whether to use the dnf4 or dnf5 backend})"
    ]
}

The above works when the target hosts are redhat since it is using yum module which is not supported in Ubuntu 

root@ip-172-31-95-88:/home/ubuntu/ansible/writing-inventory-file# ansible web01 -m ansible.builtin.apt -a "name=apache2 state=present update_cache=yes" -i inventory.yaml --become
[WARNING]: Platform linux on host web01 is using the discovered Python interpreter at
/usr/bin/python3.12, but future installation of another Python interpreter could change the meaning of
that path. See https://docs.ansible.com/ansible-
core/2.18/reference_appendices/interpreter_discovery.html for more information.
web01 | CHANGED => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.12"
    },
    "cache_update_time": 1751272400,
    "cache_updated": true,
    "changed": true,
    "stderr": "",


root@ip-172-31-95-88:/home/ubuntu/ansible/writing-inventory-file# ansible webservers -m ansible.builtin.apt -a "name=apache2 state=present update_cache=yes" -i inventory.yaml --become
-> Only web02 will be changed not web01 

root@ip-172-31-95-88:/home/ubuntu/ansible/writing-inventory-file# ansible webservers -m ansible.builtin.apt -a "name=apache2 state=present update_cache=yes" -i inventory.yaml --become

web01 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.12"
    },
    "cache_update_time": 1751272622,
    "cache_updated": true,
    "changed": false
}

web02 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.12"
    },
    "cache_update_time": 1751272622,
    "cache_updated": true,
    "changed": false



================= Playbooks ======================
Playbooks are written in yaml format

Example: 

- hosts: websrvgrp           # First play
  tasks:
  - name: Install Apache      # 1st Tasks of 1st play
    yum:
	  name: apache2
	  state: present 
  - name: Deploy Config      # 2nd Task of 2nd play
    copy: 
	  src: file/httpd.conf
	  dest: /etc/httpd.conf
- hosts: dbsvrgrp			 #	Second Play 
  tasks:             
  - name: Install Postresql  # 1st Task of 2nd play 
    yum: 
	  name: postgresql
	  state: latest


================= VARIABLES =========================================
There are diff ways in which we can define the variables 

1. Using Variables in the Playbook 
2. Using Varibales for Inventory 
3. Using Variables in setup module 
4. Using variables in Register Module

-------------------
Using Variable in plabook
-------------------
- hosts: websrvgrp
  vars: 
    http_port: 80 
    sqluser.admin 


------------------------------------
Inventory Based - In inventory file 
------------------------------------
group_vars/all (Cur Dir) 	      --> The variables can be used for all the hosts 
group_vars/groupname (Cur Dir) --> The variables can be used for the groupnames
host_vars/hostname (Cur Dir)    --> The variables can be used for the partiular hosts 

----------
Roles
----------
Include variables from files in playbook 

--------------------------------
Setup Module - Runtime variable
--------------------------------
When we run a ansible playbook first it always runs the first task as gathering facts will run the setup module and generate the fact variables

ansible_os_familiy
OS name liek Redhat, Debain 

ansible_process_corese
Number of CPU Cores 

ansible_kernal 
Kernel Version 

ansible_devices
conneced Device information 

ansible_default_inpv4
IP MAC address, gateway etc 

ansible architecture
64 bit or 32 bit 

These facts variables can be used inside the playbook to check these kind of variables and run a conditional taks

-----------------------------------------------
Store Ouput: Register Module - Runtime Variable
-----------------------------------------------
Module execution(Playbook Tasks) -> Return Output(JSON FORMAT)-> Store Output(Variable to store) ->  Use Variable (In Next Tasks/Print it) 


In Plyabook 
- name: DBServer Setup 
  hosts: dbservers
  become: yes 
  vars: 
    dbname: arjun 
    dbuser: current
    dbpass: tesla
  tasks
   - debug
       msg: "The DBNAME is {{dbname}}" # For priniting the variable. this is like a print statement like any other languages 

   - name: Install mariadb-server 
     ansible.builtin.apt
       name: mariadb-server
       state: present 

   - name: Install pymysql 
     ansible.builtin.apt
       name: python3-PyMySql 
       state: present 

   - name: Strt mariadb service 
     ansible.builtinserviec 
       name: mariadb
       state: started
       enabled: yes

   - name: Create a new database with 'accounts' 
     community.mysql.mysql_db:
        name: "{{dbname}}"   # Usage of variable 
        state: present
        login_unix_socket: /var/lib/mysql/mysql.sock
     
    - name: Create database user with name 'arjun'
      community.mysql.mysql_user: 
        name: "{{dbuser}}"
        password: "{{dbpass}}"
        priv: '*.*:ALL'
	state: present
	login_unix_socket: /var/lib/mysql/mysql.lock
      register: dbout # To get the output

     - name: print dbout variable 
       debug: 
         var: dbout
      -name: print name 
       debug: 
         var: dbout.name 

$ ansible-playbook -i inventory.yaml db-creation.yaml -vv

Note: 
1. Priority will be highest if you have declared variables inside the plyabook, even when you have group/all, etc in the current dir. (If we pass in cmd that has more priority but usually will using vars in the yaml itself)
Example for cmd: ansible-playbook -e USRNME=newuser -e COMM=clieuser Create_user.yaml  
2. When we declare variables outide the playbook the priority is as follows 
     i. group_vars/hostname
     ii. group_vars/groups
     iii. group_vars/all 


----------- Create_user.yaml (Precedence of var declaration) -------

- name: Understanding vars
  hosts: all 
  become: yes 
    #vars: 
      #USRM: playuser
      #COMM: variable form playbook 
   tasks: 
     - name: Create User
       ansible.builtin.user
         name: "{{USRNM}}"
         comment: "{{COMM}}"
       register: usrout 
      - debug
          var: userout.name 
      - debug
          var: userout.comment
-------------------------------------------------------------------

$cat group_vars/all --> It will be applied to all 
USRNM: commonuser
COMM: variable from groupvars_all file 
 
$cat groups_vars/webservers  --> it will applied to webserver
USRNM: webgroup 
COMM: variable from group_vars/webserver file 

$ ls
ansible.cfg clientKey.pem group_vars/ inventory.yaml db.yaml 
Similarly for groups_hostaname

$ cat groups_vars/web01 --> It will be applied to web01
USERNM: web02user
COMM: variables from host_vars/web02 file 

$ ansible-playbook -e USRNME=cliuser -e COMM=clieuser Create_user.yaml 
OUTPUT: 

TASK [debug] ************************************************************************
ok [web01] => {
   "userout.name": "cliuser"
}
ok: [web02] => {
    "usrout.name": "cliuser"
}
ok: [db01] => { 
    "userout.name": "cliuser"
} 

TASK [debug] *************************************************************************
ok [web01] => {
   "userout.comment": "cliuser"
}
ok: [web02] => {
    "usrout.comment": "cliuser"
}
ok: [db01] => { 
    "userout.comment": "cliuser"
} 

-------- FACT Variables: SETUP MODULE ----------------------

When we run a ansible playbook first it always runs the first task as gathering facts that will run the setup module and generate the fact variables (gloabl Variables)

ansible_os_familiy
OS name liek Redhat, Debain 

ansible_process_corese
Number of CPU Cores 

ansible_kernal 
Kernel Version 

ansible_devices
conneced Device information 

ansible_default_inpv4
IP MAC address, gateway etc 

ansible architecture
64 bit or 32 bit 

These facts variables can be used inside the playbook to check these kind of variables and run a conditional taks

To see what are the variables which are generated by the setup module 
$ ansible -m setup web01

======== print_facts.yaml ======================

- name: Print Facts 
  hosts: all 
  gather_facts: True
  tasks: 
   - name: print OS name 
     debug: 
       var: ansible_distribution # Access Dictionary

   - name: print RAM Memory 
     debug: 
       var: ansibl_memory_mv.real.free # Access Dictionary

    - name: print Processor Memory  
     debug: 
       var: ansible_processor[2]   # Access a List

$ ansible-playbook print_facts_yaml 

================= Provisioning Server ================

1. NTP Service on mutli OS
2. Users & Groups 
3. Configuration files
4. Decision Making
5. Loops
6. Templates 
7. Handlers
8. Ansible Roles

$ ansible -m setup web01
        "ansible_distribution": "Ubuntu",
        "ansible_distribution_file_parsed": true,
        "ansible_distribution_file_path": "/etc/os-release",
        "ansible_distribution_file_variety": "Debian",
        "ansible_distribution_major_version": "24",
        "ansible_distribution_release": "noble",
        "ansible_distribution_version": "24.04",

---------------  provisioning.yaml -----------------------

- name: Provisioning servers
  hosts: all
  become: yes
  tasks:
    - name: Install chrony
      package:
        name: chrony
        state: present
      when: ansible_os_family in ["Debian", "RedHat"]

    - name: Start and enable chrony
      service:
        name: "{{ 'chronyd' if ansible_os_family == 'RedHat' else 'chrony' }}"
        state: started
        enabled: yes

------- Loops -----------------
- name: Provisioning servers
  hosts: all
  become: yes
  tasks:
    - name: Install chrony
      package:
        name: "{{item}}"
        state: present
      when: ansible_os_family in ["Debian", "RedHat"]
      loop:
        - chrony
        - wget
        - git
        - zip
        - unzip

    - name: Start and enable chrony
      service:
        name: "{{ 'chronyd' if ansible_os_family == 'RedHat' else 'chrony' }}"
        state: started
        enabled: yes

Output: 
ubuntu@ip-172-31-95-88:~/ansible/writing-inventory-file$ ansible-playbook loops.yaml
[WARNING]: log file at '/home/ubuntu/ansible/writing-inventory-file/var/log/ansible.log' is not writeable and we cannot create it, aborting


PLAY [Provisioning servers] ****************************************************

TASK [Gathering Facts] *********************************************************
[WARNING]: Platform linux on host web01 is using the discovered Python
interpreter at /usr/bin/python3.12, but future installation of another Python
interpreter could change the meaning of that path. See
https://docs.ansible.com/ansible-
core/2.18/reference_appendices/interpreter_discovery.html for more information.
ok: [web01]
[WARNING]: Platform linux on host db01 is using the discovered Python
interpreter at /usr/bin/python3.12, but future installation of another Python
interpreter could change the meaning of that path. See
https://docs.ansible.com/ansible-
core/2.18/reference_appendices/interpreter_discovery.html for more information.
ok: [db01]
[WARNING]: Platform linux on host web02 is using the discovered Python
interpreter at /usr/bin/python3.12, but future installation of another Python
interpreter could change the meaning of that path. See
https://docs.ansible.com/ansible-
core/2.18/reference_appendices/interpreter_discovery.html for more information.
ok: [web02]

TASK [Install chrony] **********************************************************
ok: [web02] => (item=chrony)
ok: [web01] => (item=chrony)
ok: [db01] => (item=chrony)
ok: [web02] => (item=wget)
ok: [web01] => (item=wget)
ok: [db01] => (item=wget)
ok: [web02] => (item=git)
ok: [web01] => (item=git)
ok: [db01] => (item=git)
changed: [web01] => (item=zip)
changed: [web02] => (item=zip)
ok: [web01] => (item=unzip)
changed: [db01] => (item=zip)
ok: [web02] => (item=unzip)
ok: [db01] => (item=unzip)

TASK [Start and enable chrony] *************************************************
ok: [web01]
ok: [web02]
ok: [db01]

PLAY RECAP *********************************************************************
db01                       : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
web01                      : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
web02                      : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0

=========== File Operations =====================
Helpful if you go through the Ansible documentations

- name: Provisioning servers
  hosts: all
  become: yes
  tasks:
    - name: Install chrony
      package:
        name: chrony
        state: present
      when: ansible_os_family in ["Debian", "RedHat"]

    - name: Start and enable chrony
      service:
        name: "{{ 'chronyd' if ansible_os_family == 'RedHat' else 'chrony' }}"
        state: started
        enabled: yes

    - name: Banner file
      copy:
        content: "# This Server is Managed by ansible. Please don't perform manuall changes"
        dest: /etc/motd


    - name: Deploy ntp agent conf on Ubuntu/Centos
      template:
        src: templates/ntconf
        dest: /etc/chrony.conf
        backup: yes
      when: ansible_os_family in ["Debain", "Redhat"]


    - name: retart and enable chrony
      service:
        name: "{{ 'chronyd' if ansible_os_family == 'RedHat' else 'chrony' }}"
        state: restarted                                                           enabled: yes



================  Ansible Roles =============================

Ansible Roles is simplify the playbook content
We have currently: 

|-- ansible.cfg
|-- inventory.yaml
|-- post_install.yml
|-- files/
|   â””â”€â”€ datafiles.tar.gz
|-- templates/
|   â”œâ”€â”€ ntp_conf_redhat.j2
|   â””â”€â”€ ntp_conf_ubuntu.j2


To make this more managable we can use ansible Rolese

--------------- Example Roles DIR-----------------
roles/  --> it is useful for organization level 
â””â”€â”€ myrole/
    â”œâ”€â”€ defaults/
    â”‚   â””â”€â”€ main.yml          # Default variables for the role (lowest precedence)
    â”œâ”€â”€ files/
    â”‚   â””â”€â”€ some_file.conf    # Static files to be copied to the target
    â”œâ”€â”€ handlers/
    â”‚   â””â”€â”€ main.yml          # Handlers triggered by `notify`
    â”œâ”€â”€ meta/
    â”‚   â””â”€â”€ main.yml          # Role metadata (e.g., dependencies)
    â”œâ”€â”€ tasks/
    â”‚   â””â”€â”€ main.yml          # Main list of tasks to run
    â”œâ”€â”€ templates/
    â”‚   â””â”€â”€ config.j2         # Jinja2 templates to be rendered
    â”œâ”€â”€ vars/
    â”‚   â””â”€â”€ main.yml          # Variables with higher precedence than defaults
    â””â”€â”€ README.md             # (Optional) Documentation for the role

----- Non Role based yaml structure -----------

- name: Provisioning servers
  hosts: all
  become: yes 
  vars: 
    dir: /opt/dir2 

  tasks:
    - name: Install chrony
      package:
        name: chrony
        state: present
      when: ansible_os_family in ["Debian", "RedHat"]

    - name: Start and enable chrony
      service:
        name: "{{ 'chronyd' if ansible_os_family == 'RedHat' else 'chrony' }}"
        state: started
        enabled: yes

    - name: Banner file
      copy:
        content: "# This Server is Managed by ansible. Please don't perform manuall changes"
        dest: /etc/motd


    - name: Deploy ntp agent conf on Ubuntu/Centos
      template:
        src: templates/ntconf
        dest: /etc/chrony.conf
        backup: yes
      when: ansible_os_family in ["Debain", "Redhat"]

    - name: Create a Dir
      file: 
        path: "{{dir}}"
        state: directory 

    - name: Dump file 
      copy: 
        src: files/myfile.txt
        dest: /tmp/myfile.txt

    - name: retart and enable chrony
      service:
        name: "{{ 'chronyd' if ansible_os_family == 'RedHat' else 'chrony' }}"
        state: restarted                                                           enabled: yes

So this can be minimized and more correct way is to define using roles and following the dir format for handlers, tasks, files, ect.

#############################################################
# ðŸ“˜ ANSIBLE ROLES â€“ COMPLETE NOTES + EXAMPLES
#############################################################

ðŸ§  WHAT IS A ROLE?
-------------------
A role is a set of tasks, variables, handlers, files, and templates
grouped together for reusability and better organization.

Used to modularize complex playbooks.

-------------------------------------------------------------
ðŸ“ STANDARD DIRECTORY STRUCTURE FOR ROLES
-------------------------------------------------------------

project/
â”œâ”€â”€ ansible.cfg
â”œâ”€â”€ inventory.yaml
â”œâ”€â”€ site.yml                    # Main playbook
â””â”€â”€ roles/
    â””â”€â”€ ntp/
        â”œâ”€â”€ defaults/
        â”‚   â””â”€â”€ main.yml        # Default (lowest priority) vars
        â”œâ”€â”€ files/
        â”‚   â””â”€â”€ datafiles.tar.gz  # Static files
        â”œâ”€â”€ handlers/
        â”‚   â””â”€â”€ main.yml        # Handlers triggered by notify
        â”œâ”€â”€ tasks/
        â”‚   â””â”€â”€ main.yml        # Main task list
        â”œâ”€â”€ templates/
        â”‚   â”œâ”€â”€ ntp_conf_redhat.j2
        â”‚   â””â”€â”€ ntp_conf_ubuntu.j2
        â”œâ”€â”€ vars/
        â”‚   â””â”€â”€ main.yml        # High-priority variables
        â””â”€â”€ meta/
            â””â”€â”€ main.yml        # Role metadata (optional)

-------------------------------------------------------------
ðŸ› ï¸ CREATING A ROLE USING COMMAND LINE
-------------------------------------------------------------

# Run this in your Ansible project directory:
ansible-galaxy init roles/ntp

# It creates the role skeleton under roles/ntp/

-------------------------------------------------------------
âœï¸ EXAMPLE: roles/ntp/tasks/main.yml
-------------------------------------------------------------

---
- name: Install chrony on Ubuntu
  apt:
    name: chrony
    state: present
    update_cache: yes
  when: ansible_os_family == "Debian"

- name: Install chrony on RedHat
  yum:
    name: chrony
    state: present
  when: ansible_os_family == "RedHat"

- name: Configure NTP (template)
  template:
    src: "{{ template_file }}"
    dest: /etc/chrony.conf
  notify: Restart chrony

- name: Ensure chrony is running
  service:
    name: "{{ chrony_service }}"
    state: started
    enabled: yes

-------------------------------------------------------------
ðŸ“¦ Example: roles/ntp/templates/ntp_conf_ubuntu.j2
-------------------------------------------------------------
# NTP config for Ubuntu
server 0.ubuntu.pool.ntp.org iburst
server 1.ubuntu.pool.ntp.org iburst

-------------------------------------------------------------
ðŸ“¦ Example: roles/ntp/templates/ntp_conf_redhat.j2
-------------------------------------------------------------
# NTP config for RHEL/CentOS
server 0.centos.pool.ntp.org iburst
server 1.centos.pool.ntp.org iburst

-------------------------------------------------------------
ðŸ”§ Example: roles/ntp/handlers/main.yml
-------------------------------------------------------------

---
- name: Restart chrony
  service:
    name: "{{ chrony_service }}"
    state: restarted

-------------------------------------------------------------
ðŸ“„ Example: roles/ntp/defaults/main.yml
-------------------------------------------------------------

---
chrony_service: "{{ 'chronyd' if ansible_os_family == 'RedHat' else 'chrony' }}"
template_file: "{{ 'ntp_conf_redhat.j2' if ansible_os_family == 'RedHat' else 'ntp_conf_ubuntu.j2' }}"

-------------------------------------------------------------
ðŸ“„ Example: site.yml (Main Playbook)
-------------------------------------------------------------

---
- name: Configure NTP using a role
  hosts: all
  become: yes
  roles:
    - ntp

-------------------------------------------------------------
ðŸ“„ Example: inventory.yaml
-------------------------------------------------------------

all:
  hosts:
    server1:
      ansible_host: 192.168.0.101
    server2:
      ansible_host: 192.168.0.102

-------------------------------------------------------------
âš™ï¸ Example: ansible.cfg
-------------------------------------------------------------

[defaults]
inventory = inventory.yaml
roles_path = ./roles
host_key_checking = False

-------------------------------------------------------------
ðŸš€ RUNNING THE PLAYBOOK
-------------------------------------------------------------

ansible-playbook site.yml

-------------------------------------------------------------
âœ… RESULT
-------------------------------------------------------------
- Chrony installed based on OS
- Correct NTP config template used
- Service enabled and restarted if config changes

#############################################################
# END OF Rols
#############################################################

================== AWS Automation Tasks ===========================
#############################################################
ðŸ“˜ ANSIBLE NOTES: CREATE AWS EC2 KEY PAIR & SAVE LOCALLY
#############################################################

ðŸ§° REQUIREMENTS
---------------
1. Install `boto3` and `botocore` (Python AWS SDKs):
   pip install boto3 botocore

2. Install AWS Ansible collection (optional but recommended):
   ansible-galaxy collection install amazon.aws

3. AWS credentials setup:
   Either use:
   - ~/.aws/credentials (default profile)
   - Set environment variables:
     export AWS_ACCESS_KEY_ID="your-access-key"
     export AWS_SECRET_ACCESS_KEY="your-secret-key"
     export AWS_REGION="your-region"

4. Ensure your `ansible.cfg` allows community modules if needed.

-------------------------------------------------------------
ðŸ” STEP 1: CREATE ANSIBLE PLAYBOOK (keypair.yml)
-------------------------------------------------------------

---
- name: Create and store an AWS EC2 key pair
  hosts: localhost
  connection: local
  gather_facts: no
  vars:
    key_name: my-ansible-key
    key_path: "./{{ key_name }}.pem"
  tasks:

    - name: Create EC2 key pair
      amazon.aws.ec2_key:
        name: "{{ key_name }}"
        region: "{{ lookup('env', 'AWS_REGION') }}"
        state: present
      register: keypair_result

    - name: Save private key to file
      copy:
        content: "{{ keypair_result.key.private_key }}"
        dest: "{{ key_path }}"
        mode: '0600'
      when: keypair_result.changed

    - name: Print success message
      debug:
        msg: "Key pair '{{ key_name }}' created and saved to {{ key_path }}"

-------------------------------------------------------------
ðŸ“ OUTPUT
-------------------------------------------------------------
If successful, a file named `my-ansible-key.pem` will be created in your current directory.
This file can be used to SSH into EC2 instances launched with this key.

-------------------------------------------------------------
ðŸ“› ERROR HANDLING
-------------------------------------------------------------
- If the key already exists, Ansible will not overwrite it.
- If AWS credentials are incorrect, boto3 will raise an error.

-------------------------------------------------------------
âœ… COMMAND TO EXECUTE THE PLAYBOOK
-------------------------------------------------------------

ansible-playbook keypair.yml

-------------------------------------------------------------
ðŸ“Œ NOTES
-------------------------------------------------------------
- Only the first time you create the key, AWS provides the private key.
- If you lose the `.pem`, you cannot recover it â€” you'll need to create a new key.
- Always keep `.pem` files secure and `chmod 600` protected.

#############################################################
# END OF CREATING KEY PAIR 
#############################################################

#############################################################
ðŸ“˜ ANSIBLE PLAYBOOK: CREATE KEY PAIR & LAUNCH EC2 INSTANCE
#############################################################

---
- name: Create key pair and launch EC2 with internet access
  hosts: localhost
  connection: local
  gather_facts: no

  vars:
    region: "{{ lookup('env', 'AWS_REGION') | default('us-east-1') }}"
    key_name: "my-ansible-key"
    key_path: "./{{ key_name }}.pem"
    ami_id: "ami-0c02fb55956c7d316"         # Ubuntu 22.04 in us-east-1
    instance_type: "t2.micro"
    security_group_name: "ansible-sg"
    instance_name: "AnsibleEC2Instance"

  tasks:

    - name: Create EC2 key pair
      amazon.aws.ec2_key:
        name: "{{ key_name }}"
        region: "{{ region }}"
        state: present
      register: keypair_result

    - name: Save private key to .pem file
      copy:
        content: "{{ keypair_result.key.private_key }}"
        dest: "{{ key_path }}"
        mode: '0600'
      when: keypair_result.changed

    - name: Create security group for SSH
      amazon.aws.ec2_group:
        name: "{{ security_group_name }}"
        description: "Allow SSH access"
        region: "{{ region }}"
        rules:
          - proto: tcp
            ports: [22]
            cidr_ip: 0.0.0.0/0
        rules_egress:
          - proto: -1
            cidr_ip: 0.0.0.0/0
        state: present

    - name: Launch EC2 instance
      amazon.aws.ec2_instance:
        name: "{{ instance_name }}"
        region: "{{ region }}"
        key_name: "{{ key_name }}"
        instance_type: "{{ instance_type }}"
        image_id: "{{ ami_id }}"
        wait: yes
        count: 1
        network:
          assign_public_ip: yes
          security_group: "{{ security_group_name }}"
        instance_tags:
          Name: "{{ instance_name }}"
        state: running
      register: ec2_result

    - name: Output public IP of instance
      debug:
        msg: "EC2 launched! Public IP: {{ ec2_result.instances[0].public_ip_address }}"

#############################################################
âœ… COMMAND TO EXECUTE
-------------------------------------------------------------
ansible-playbook ec2_with_keypair.yml

#############################################################
ðŸ“¦ REQUIREMENTS
-------------------------------------------------------------
- Install required Python libraries:
    pip install boto3 botocore
- Set AWS credentials:
    export AWS_ACCESS_KEY_ID=...
    export AWS_SECRET_ACCESS_KEY=...
    export AWS_REGION=us-east-1
- Install AWS Ansible collection:
    ansible-galaxy collection install amazon.aws

#############################################################
# END OF NOTES
#############################################################



============Ansible Terms========================================

1.Controller Machine: where the Ansible is intalled and has the provsion to maintain other nodes.
2.Inventory: An initilization file where information about other nodes are present.
3.Playbook: where the automation is defined through tasks using YAML Files.
4.Task: block that defines single procedure to executed eg. Install a Package.
5.Module: It abstracts a system task, like dealing with packages or creating and changing files.
6.Role: A pre-defined way for organizing playbooks and other files in order to facililate sharing and reusing portions of a provisioning.
7.Play: Execution of a playbook is called play.
8.Facts: Global variables containing information about the system,like network interface or operating system.
9.Handlers: Triggger a service status changes, like restart ot stop a service.
 

===========Modules==============================================
 
System -> cron,iptable,hostname,mount etc
Commands -> chdir,cmd,removes,stdin,creates
Files -> acl,achieve,copy,File,Replace
Databse -> mysql_db,users
cloud
widows and more

Ansible AD-HOC Commands
To ping to the host: ansible all -m ping -k(with passcode)
To view mem: ansible all -m shell -a 'free -m'
To create files: ansible all -m shell -a "touch /home/vunet/arjun.txt" -k(optional)(-m modules)
changing file ownership: ansible all -m file -a "dest=/home/vunet/arjun.txt mode=700"(-a arguements can m=be multiple)
ansible all -m shell -a "ls -ltr /home/vunet/ | grep arjun" (shell for output in terminal)
ansible all -b -m file -a "dest=/home/vunet/arjun.txt mode=777 owner=root group=root" (-b temporary root)
ansible all -b -m file -a "dest=/home/vunet/arjun state=directory" -k


======To run a playbook:================= 
ansible playbook <playbook name> 

=======To run the playbook in background================
nohup ansible-playbook docker_service_check_async.yaml > ansible_output.log 2>&1 &

nohup: Runs the command in the background and ignores hangup (HUP) signals, so it continues to run even if the terminal is closed.
>: Redirects standard output to a file (ansible_output.log in this case).
2>&1: Redirects standard error to the same file as standard output.
&: Runs the command in the background.


==========Cron Job for running Playbook -> run_ansible.sh===============
#!/bin/bash

cd /path/to/your/playbook
ansible-playbook docker_service_check_async.yaml >> ansible_output.log 2>&1
*/5 * * * * /path/to/your/script/run_ansible.sh

To Remove the entire cron-Job -> cronjob -r 
To view the existing cron-Jov -> cronjob -l
==========================================================================
chmod +x run_ansible.sh
crontab -e
/var/log/syslog or /var/log/cron -> for logs
==================================================
---
- name: Check Docker Service Status
  hosts: localhost
  gather_facts: False
  tasks:
    - name: Run docker service ls command asynchronously
      shell: docker service ls --format "{{ '{{' }}.Name{{ '}}' }}:{{ '{{' }}.Replicas{{ '}}' }}"
      register: docker_service_result
      async: 300
      poll: 5

    - name: Print async status
      async_status:
        jid: "{{ docker_service_result.ansible_job_id }}"
      register: job_result
      until: job_result.finished
      retries: 300
      delay: 5

    - name: Display Docker service status
      debug:
        msg: "Service {{ item.split(':')[0] }} is {{ 'running' if item.split(':')[1] == '1/1' else 'not running' }}"
      loop: "{{ docker_service_result.stdout_lines.results }}"

====================================================
Above playbook will give job-id 
ansible-playbook docker_service_check_async.yaml
ansible localhost -m async_status -a "jid=<ansible_job_id>" 
By using the async_status module, you can check the progress and results of the background task while it's running. Adjust the playbook and monitoring parameters based on your specific needs
==================================================================
===========Async playbook ========


- name: Check Docker Service Status and Run Script
  hosts: 164.52.200.58
  gather_facts: False
  tasks:
    - name: Run docker service ls command asynchronously
      shell: docker service ls --format "{{ '{{' }}.Name{{ '}}' }}:{{ '{{' }}.Replicas{{ '}}' }}"
      register: docker_service_result
      async: 300
      poll: 5

    - name: Print async status
      async_status:
        jid: "{{ docker_service_result.ansible_job_id }}"
      register: job_result
      until: job_result.finished
      retries: 300
      delay: 5

    - name: Check Docker service status and run script if not running
      shell: |
        container_status="{{ docker_service_result.stdout_lines.results[0].item.split(':')[1] }}"
        container_name="{{ docker_service_result.stdout_lines.results[0].item.split(':')[0] }}"
        if [[ "{{ container_name }}" == "enrichment-preprocessor" && "{{ container_status }}" == "0/1" ]]; then
          ./create-stream-v1.sh deployment-configs/{{ container_name }}.sh
        fi
      args:
        executable: /bin/bash
      when: docker_service_result.stdout_lines.results[0].finished
      become: true
      become_user: vunet

    - name: Display Docker service status
      debug:
        msg: "Service {{ item.split(':')[0] }} is {{ 'running' if item.split(':')[1] == '1/1' else 'not running' }}"
      loop: "{{ docker_service_result.stdout_lines.results }}"


===============================================================================================================================

---
- name: Check Docker Service Status and Run Script
  hosts: 164.52.200.58
  gather_facts: False
  tasks:
    - name: Run docker service ls command
      shell: docker service ls --format "{{ '{{' }}.Name{{ '}}' }}:{{ '{{' }}.Replicas{{ '}}' }}"
      register: docker_service_result

    - name: Display Docker service status and run script if not running
      block:
        - name: Display Docker service status
          debug:
            msg: "Service {{ item.split(':')[0] }} is {{ 'running' if item.split(':')[1] == '1/1' else 'not running' }}"
          loop: "{{ docker_service_result.stdout_lines }}"

        - name: Check Docker service status and run script if not running
          shell: |
            container_name = "{{ item.split(':')[0] }}"
            container_status = "{{ item.split(':')[1] }}"
            if [[ "{{ container_name }}" == "enrichment-preprocessor" && "{{ container_status }}" == "0/1" ]]; then
              ./create-stream-v1.sh deployment-configs/{{ container_name }}.sh
            fi
          args:
            executable: /bin/bash
          with_items: "{{ docker_service_result.stdout_lines }}"
          when: "'enrichment-preprocessor:0/1' in item"
          become: true
          become_user: vunet

=================================================================================================================================

---
- name: Check Docker Service Status and Run Script
  hosts: 164.52.200.58
  gather_facts: False
  tasks:
    - name: Run docker service ls command
      shell: docker service ls --format "{{ '{{' }}.Name{{ '}}' }}:{{ '{{' }}.Replicas{{ '}}' }}"
      register: docker_service_result

    - name: Display Docker service status and run script if not running
      block:
        - name: Display Docker service status
          debug:
            msg: "Service {{ item.split(':')[0] }} is {{ 'running' if item.split(':')[1] == '1/1' else 'not running' }}"
          loop: "{{ docker_service_result.stdout_lines }}"

        - name: Check Docker service status and run script if not running
          shell: |
            container_name="{{ item.split(':')[0] }}"
            container_status="{{ item.split(':')[1] }}"

            script_path="/data/vusmartmaps/bin/kafka-streams/create-stream-v1.sh"
            deployment_path="/data/vusmartmaps/bin/kafka-streams/deployment-configs/"

            if [[ "$container_status" == "0/1" ]]; then
              "$script_path" "$deployment_path"/"$container_name".sh
            fi
          args:
            executable: /bin/bash
          with_items: "{{ docker_service_result.stdout_lines }}"
          become: true
          become_user: vunet

---------------------------------------------------------------------------------
---
- name: Restart or Delete Kafka Connect Connectors if FAILED
  hosts: 164.52.200.58
  become: yes
  tasks:
    - name: Get list of running Kafka Connect connectors
      command: "curl -s -X GET http://localhost:9082/connectors"
      register: connector_names

    - name: Identify connectors in FAILED state
      command: "curl -s -X GET http://localhost:9082/connectors/{{ item }}/status | jq -r '[.tasks[]? | select(.state == \"FAILED\").name] | length > 0 // empty' "
      register: failed_connectors
      failed_when: false
      with_items: "{{ connector_names.stdout | from_json }}"

    - name: Debug Failed Connectors
      ansible.builtin.debug:
        var: failed_connectors.results | map(attribute='stdout') | list

    # Add your tasks to delete or restart connectors based on the failed state

-----------------------------------------------------------------------------------
---
- name: Restart or Delete Kafka Connect Connectors if FAILED
  hosts:  164.52.200.58
  become: yes
  tasks:
    - name: Get list of running Kafka Connect connectors
      command: "curl -s -X GET http://localhost:9082/connectors | jq"
      register: connector_names

    - name: Identify connectors in FAILED state
      command: "curl -s -X GET http://localhost:9082/connectors/{{ item }}/status | jq -r '.tasks[] | select(.state == \"FAILED\").state' "
      register: connector_state
      failed_when: "connector_state.stdout == 'FAILED'"
      with_items: "{{ connector_names.stdout | from_json }}"

    - name: Delete connectors in FAILED state
      command: "curl -X DELETE http://localhost:9082/connectors/{{ item }}"
      with_items: "{{ connector_state.results | selectattr('stdout', 'equalto', 'FAILED') | map(attribute='item') | list }}"
      when: connector_state.results | selectattr('stdout', 'equalto', 'FAILED') | list | count > 0

    - name: Change directory to the connector script location
      ansible.builtin.command: 
        cmd: "cd /data/vusmartmaps/bin/kafka-connect/sink-connectors/"
      when: connector_names.stdout | from_json | length > 0

    - name: Run bash script for connectors
      ansible.builtin.command: 
        cmd: "bash {{ item }}.txt"
      with_items: "{{ connector_state.results | selectattr('stdout', 'equalto', 'FAILED') | map(attribute='item') | list }}"
      when: connector_names.stdout | from_json | length > 0

--------------------------------------------------------------------------------------------------

---
- name: Check Docker Service Status and Run Script
  hosts: 164.52.200.58
  gather_facts: False
  tasks:
    - name: Run docker service ls command
      shell: docker service ls --format "{{ '{{' }}.Name{{ '}}' }}:{{ '{{' }}.Replicas{{ '}}' }}"
      register: docker_service_result

    - name: Display Docker service status and run script if not running
      block:
        - name: Display Docker service status
          debug:
            msg: "Service {{ item.split(':')[0] }} is {{ 'running' if item.split(':')[1] == '1/1' else 'not running' }}"
          loop: "{{ docker_service_result.stdout_lines }}"

        - name: Check Docker service status and run script if not running
          shell: |
            container_name="{{ item.split(':')[0] }}"
            container_status="{{ item.split(':')[1] | regex_replace('/.*', '') }}"

            script_path="/data/vusmartmaps/bin/kafka-streams/create-stream-v1.sh"
            deployment_path="/data/vusmartmaps/bin/kafka-streams/deployment-configs/"

            # Additional condition to check the container name
            if [[ "$container_status" == "0" && ! ( "$container_name" == 'vsmaps_alert-1' || "$container_name" == 'vsmaps_broker-1' || "$container_name" == 'vsmaps_connect-1' || "$container_name" == 'vsmaps_dao' || "$container_name" == 'vsmaps_elasticsearch-1' || "$container_name" == 'vsmaps_elasticsearch-2' || "$container_name" == 'vsmaps_elasticsearch-3' || "$container_name" == 'vsmaps_heartbeat-1-1-1' || "$container_name" == 'vsmaps_ksqldb-cli' || "$container_name" == 'vsmaps_ksqldb-server' || "$container_name" == 'vsmaps_mysql-1' || "$container_name" == 'vsmaps_orchestration' || "$container_name" == 'vsmaps_route' || "$container_name" == 'vsmaps_vuinterface-1' || "$container_name" == 'vsmaps_zookeeper-1' || "$container_name" == 'vusmartmaps-application' ) ]]; then
              "$script_path" "$deployment_path"/"$container_name".sh
            fi
          args:
            executable: /bin/bash
          with_items: "{{ docker_service_result.stdout_lines }}"
          become: true
          become_user: vunet

---------------------------------------------------------------------------------------------------------
---
- name: Restart or Delete Kafka Connect Connectors if FAILED
  hosts:  164.52.200.58
  become: yes
  tasks:
    - name: Get list of running Kafka Connect connectors
      command: "curl -s -X GET http://localhost:9082/connectors | jq"
      register: connector_names

    - name: Identify connectors in FAILED state
      command: "curl -s -X GET http://localhost:9082/connectors/{{ item }}/status | jq -r '.tasks[] | select(.state == \"FAILED\").state' "
      register: connector_state
      failed_when: "connector_state.stdout == 'FAILED'"
      with_items: "{{ connector_names.stdout | from_json }}"		

    - name: Delete connectors in FAILED state
      command: "curl -X DELETE http://localhost:9082/connectors/{{ item }}"
      with_items: "{{ connector_state.results | selectattr('stdout', 'equalto', 'FAILED') | map(attribute='item') | list }}"
      when: connector_state.results | selectattr('stdout', 'equalto', 'FAILED') | list | count > 0

    - name: Change directory to the connector script location
      ansible.builtin.command: 
        cmd: "cd /data/vusmartmaps/bin/kafka-connect/sink-connectors/"
      when: connector_names.stdout | from_json | length > 0

    - name: Run bash script for connectors
      ansible.builtin.command: 
        cmd: "bash {{ item }}.txt"
      with_items: "{{ connector_state.results | selectattr('stdout', 'equalto', 'FAILED') | map(attribute='item') | list }}"
      when: connector_names.stdout | from_json | length > 0

========================================================================================================================
---
- name: Check Docker Service Status and Run Script
  hosts: Analyzers
  gather_facts: False
  tasks:
    - name: Run docker service ls command
      shell: docker service ls --format "{{ '{{' }}.Name{{ '}}' }}:{{ '{{' }}.Replicas{{ '}}' }}"
      register: docker_service_result

    - name: Display Docker service status
      debug:
        msg: "Service {{ item.split(':')[0] }} is {{ 'running' if item.split(':')[1] == '1/1' else 'not running' }}"
      loop: "{{ docker_service_result.stdout_lines }}"

    - name: Check Docker service status and run script if not running
      shell: |
        container_name="{{ item.split(':')[0] }}"
        container_status="{{ item.split(':')[1] | regex_replace('/.*', '') }}"  # Extract only the first part of the status (before "/")

        script_path="/data/vusmartmaps/bin/kafka-streams/create-stream-v1.sh"
        deployment_path="/data/vusmartmaps/bin/kafka-streams/deployment-configs/"
        log_file_path="/home/vunet/ansible_log.txt"

        # Restricted container names
        restricted_container_names:
          - vsmaps_alert-1
          - vsmaps_broker-1
          - vsmaps_connect-1
          - vsmaps_dao
          - vsmaps_elasticsearch-1
          - vsmaps_elasticsearch-2
          - vsmaps_elasticsearch-3
          - vsmaps_heartbeat-1-1-1
          - vsmaps_ksqldb-cli
          - vsmaps_ksqldb-server
          - vsmaps_mysql-1
          - vsmaps_orchestration
          - vsmaps_route
          - vsmaps_vuinterface-1
          - vsmaps_zookeeper-1
          - vusmartmaps-application

        # Additional condition to check the container name
        if [[ "$container_status" == "0" && ! ( "${restricted_container_names[@]}" =~ $container_name ) ]]; then
            "{{ script_path }}" "{{ deployment_path }}/{{ container_name }}.sh"
            echo "Adapter - $container_name has been restarted, Please take necessary actions!!!" | mail -s "Adapter Restart Notification!!!!" arjun@vunetsystems.com arjunkarekar1002@gmail.com
        fi
      args:
        executable: /bin/bash
      loop: "{{ docker_service_result.stdout_lines }}"
      when: "'not running' in item.stdout"
      become: true
      become_user: vunet-prod
========================================================================================================================
---
- name: Check Docker Service Status and Run Script
  hosts: Analyzers
  gather_facts: False
  tasks:
    - name: Run docker service ls command
      shell: docker service ls --format "{{ '{{' }}.Name{{ '}}' }}:{{ '{{' }}.Replicas{{ '}}' }}"
      register: docker_service_result

    - name: Display Docker service status
      debug:
        msg: "Service {{ item.split(':')[0] }} is {{ 'running' if item.split(':')[1] == '1/1' else 'not running' }}"
      loop: "{{ docker_service_result.stdout_lines }}"

    - name: Check Docker service status and run script if not running
      shell: |
        container_name="{{ item.split(':')[0] }}"
        container_status="{{ item.split(':')[1] | regex_replace('/.*', '') }}"  # Extract only the first part of the status (before "/")

        script_path="/data/vusmartmaps/bin/kafka-streams/create-stream-v1.sh"
        deployment_path="/data/vusmartmaps/bin/kafka-streams/deployment-configs/"
        log_file_path="/home/vunet/ansible_log.txt"

        # Restricted container names
        restricted_container_names:
          - vsmaps_alert-1
          - vsmaps_broker-1
          - vsmaps_connect-1
          - vsmaps_dao
          - vsmaps_elasticsearch-1
          - vsmaps_elasticsearch-2
          - vsmaps_elasticsearch-3
          - vsmaps_heartbeat-1-1-1
          - vsmaps_ksqldb-cli
          - vsmaps_ksqldb-server
          - vsmaps_mysql-1
          - vsmaps_orchestration
          - vsmaps_route
          - vsmaps_vuinterface-1
          - vsmaps_zookeeper-1
          - vusmartmaps-application

        # Additional condition to check the container name
        if [[ "$container_status" == "0" && ! ( "${restricted_container_names[@]}" =~ $container_name ) ]]; then
            "{{ script_path }}" "{{ deployment_path }}/{{ container_name }}.sh"
            echo "Adapter - $container_name has been restarted, Please take necessary actions!!!" | mail -s "Adapter Restart Notification!!!!" arjun@vunetsystems.com arjunkarekar1002@gmail.com
        fi
      args:
        executable: /bin/bash
      loop: "{{ docker_service_result.stdout_lines }}"
      when: "'not running' in item.stdout"
      become: true
      become_user: vunet-prod

===========================================================

---
- name: Get and filter connector state
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Get connector status
      uri:
        url: "http://localhost:9082/connectors/onemoney-es-sink/status"  # Update with the actual connector name
        method: GET
      register: connector_status

    - name: Extract connector name for connectors with state "FAILED"
      set_fact:
        failed_connector_name: "{{ connector_status.json.name }}"
      when: connector_status.json.tasks | selectattr('state', 'equalto', 'FAILED') | list | length > 0

    - name: Print the connector name for the failed connector
      debug:
        var: failed_connector_name

=========================================================================
---
- name: Restart or Delete Kafka Connect Connectors if FAILED
  hosts: 164.52.200.58
  become: yes
  tasks:
    - name: Get list of running Kafka Connect connectors
      command: "curl -s -X GET http://localhost:9082/connectors | jq"
      register: connector_names
      failed_when: false

    - name: Print list of connectors
      debug:
        var: connector_names.stdout_lines

    - name: Identify connectors in FAILED state
      command: "curl -s -X GET http://localhost:9082/connectors/{{ item }}/status | jq -r '.tasks[] | select(.state == \"FAILED\") | .id, .state' "
      register: connector_state
      failed_when: false
      with_items: "{{ connector_names.stdout | from_json }}"
      ignore_errors: yes

    - name: Print debug information
      debug:
        var: connector_state

    - name: Print failed connectors
      debug:
        var: connector_state.stdout_lines
      when: connector_state.stdout_lines | length > 0

    - name: Delete connectors in FAILED state
      command: "curl -X DELETE http://localhost:9082/connectors/{{ item }}"
      with_items: "{{ connector_state.results | map(attribute='item') | list }}"
      when: connector_state.stdout_lines | length > 0

    - name: Change directory to the connector script location
      command:
        cmd: "cd /data/vusmartmaps/bin/kafka-connect/sink-connectors/"
      when: connector_names.stdout | from_json | length > 0

    - name: Run bash script for connectors
      command:
        cmd: "bash {{ item }}.txt"
      with_items: "{{ connector_state.results | map(attribute='item') | list }}"
      when: connector_names.stdout | from_json | length > 0

======================================================
    - name: Send Email for Memory Excess
      shell: |
        free_memory="{{ free_memory | replace('Gi','') | replace('Mi','') | float }}"
        threshold=3
        if [ $(echo "$free_memory > $threshold" | bc) -eq 1 ]; then
          echo "Excess memory usage alert" | mail -s "Available memory is $free_memory. Take action!!" arjun@vunetsystems.com
        else
          echo "Memory threshold not met. No email sent."
        fi
      ignore_errors: true


